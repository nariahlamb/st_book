#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¸–ç•Œä¹¦åˆ†ç±»å™¨ (Worldbook Classifier)
è´Ÿè´£å°†æå–çš„åŸå§‹è§„åˆ™å’Œäº‹ä»¶æ•°æ®è¿›è¡Œåˆ†ç±»ã€æ ‡è®°å’Œé¢„å¤„ç†
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Any, Tuple
from collections import defaultdict

from project_config import get_config


class WorldbookClassifier:
    """ä¸–ç•Œä¹¦æ•°æ®åˆ†ç±»å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.config = get_config()
        self.input_dir = Path(self.config.get("output.wb_responses_dir", "wb_responses"))
        self.output_dir = self.input_dir / "classified"
        self.output_dir.mkdir(exist_ok=True)
        
        # è§„åˆ™ç±»å‹æ˜ å°„
        self.rule_type_keywords = {
            "å†å²èƒŒæ™¯": ["å†å²", "èƒŒæ™¯", "èµ·æº", "è¿‡å»", "å¤ä»£", "æ–‡æ˜", "æœ«æ—¥", "ç¾å˜"],
            "ç¤¾ä¼šè§„åˆ™": ["ç¤¾ä¼š", "ç»„ç»‡", "åŠ¿åŠ›", "åä¼š", "ç­¾è¯", "è¯•ç‚¼", "ç§©åº"],
            "ç¥æ˜è®¾å®š": ["ç¥", "ç¥æ˜", "ç¥ç¥‡", "æœ€é«˜ç¥", "å¥³å¨²", "æ¯ç¥", "æ­è¥¿å„æ–¯"],
            "åœ°ç†èƒŒæ™¯": ["åœ°ç†", "ä¸–ç•Œ", "ç»´åº¦", "ç©ºé—´", "åäºŒç•Œ", "å‰¯æœ¬", "ä¼ é€"],
            "é­”æ³•ä½“ç³»": ["é­”æ³•", "æ„è¯†åŠ›", "èƒ½åŠ›", "è¿›åŒ–", "å¡ç‰Œ", "æ¦‚å¿µ"],
            "ç‰©ç†æ³•åˆ™": ["ç‰©ç†", "æ³•åˆ™", "è§„åˆ™", "ç°å®", "æ—¶ç©º", "å› æœ"],
            "ä¿®ç‚¼ä½“ç³»": ["ä¿®ç‚¼", "è¿›åŒ–è€…", "æ½œåŠ›å€¼", "ä¼™ä¼´", "æˆé•¿"],
            "ç§æ—è®¾å®š": ["ç§æ—", "äººç±»", "å •è½ç§", "è¿›åŒ–è€…", "ç”Ÿç‰©", "è¡ç”Ÿç‰©"],
            "æŠ€æœ¯æ°´å¹³": ["æŠ€æœ¯", "ç§‘æŠ€", "æ˜Ÿèˆ°", "AI", "èè±æ–¯", "å‰¯æœ¬æ„å»º"],
            "ç»æµä½“ç³»": ["ç»æµ", "è´§å¸", "æ™¶çŸ³", "çº¢æ™¶", "äº¤æ˜“", "eBay"],
            "ç”Ÿç‰©è®¾å®š": ["ç”Ÿç‰©", "æ€ªç‰©", "è¡ç”Ÿç‰©", "å‰¯æœ¬ç”Ÿç‰©", "é•¿é’è™«"]
        }
        
        # äº‹ä»¶ç±»å‹æ˜ å°„
        self.event_type_keywords = {
            "èƒ½åŠ›è§‰é†’": ["è§‰é†’", "èƒ½åŠ›", "è·å¾—", "æ¿€æ´»"],
            "ä¿®ç‚¼çªç ´": ["ä¿®ç‚¼", "çªç ´", "æå‡", "æˆé•¿"],
            "å…³ç³»å»ºç«‹": ["å…³ç³»", "ç»“ç›Ÿ", "åˆä½œ", "å‹è°Š"],
            "æˆ˜æ–—å†²çª": ["æˆ˜æ–—", "å†²çª", "å¯¹æŠ—", "æ”»å‡»"],
            "æ¢ç´¢å‘ç°": ["æ¢ç´¢", "å‘ç°", "è°ƒæŸ¥", "å¯»æ‰¾"],
            "å±æœºäº‹ä»¶": ["å±æœº", "å¨èƒ", "å±é™©", "ç¾éš¾"],
            "é‡è¦å†³ç­–": ["å†³ç­–", "é€‰æ‹©", "å†³å®š", "æŠ‰æ‹©"]
        }
        
        print(f"ğŸ“‚ åˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def classify_all(self) -> bool:
        """æ‰§è¡Œå®Œæ•´çš„åˆ†ç±»æµç¨‹"""
        try:
            print("ğŸ”„ å¼€å§‹æ‰§è¡Œä¸–ç•Œä¹¦æ•°æ®åˆ†ç±»...")
            
            # 1. åˆ†ç±»è§„åˆ™
            print("ğŸ“‹ åˆ†ç±»è§„åˆ™æ•°æ®...")
            classified_rules = self.classify_rules()
            
            # 2. åˆ†ç±»äº‹ä»¶å¹¶æå–å®ä½“
            print("ğŸ“… åˆ†ç±»äº‹ä»¶æ•°æ®...")
            classified_events, entities = self.classify_events()
            
            # 3. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            self._generate_classification_report(classified_rules, classified_events, entities)
            
            print("âœ… ä¸–ç•Œä¹¦æ•°æ®åˆ†ç±»å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ ä¸–ç•Œä¹¦æ•°æ®åˆ†ç±»å¤±è´¥: {e}")
            return False
    
    def classify_rules(self) -> Dict[str, List[Dict[str, Any]]]:
        """åˆ†ç±»è§„åˆ™æ•°æ®"""
        rules = self._load_rules()
        classified_rules = defaultdict(list)
        
        print(f"ğŸ“Š å¼€å§‹åˆ†ç±» {len(rules)} ä¸ªè§„åˆ™...")
        
        for rule in rules:
            rule_type = self._detect_rule_type(rule)
            
            # æ·»åŠ åˆ†ç±»æ ‡ç­¾
            rule["type"] = rule_type
            rule["layer"] = "rules_layer"
            rule["suggested_order"] = self._calculate_rule_order(rule_type)
            rule["comment_prefix"] = "ã€ä¸–ç•Œè§„åˆ™ã€‘"
            
            classified_rules[rule_type].append(rule)
        
        # ä¿å­˜åˆ†ç±»ç»“æœ
        self._save_classified_data("classified_rules.json", dict(classified_rules))
        
        print(f"âœ… è§„åˆ™åˆ†ç±»å®Œæˆï¼Œå…± {len(classified_rules)} ç§ç±»å‹")
        return dict(classified_rules)
    
    def classify_events(self) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, Dict[str, Any]]]:
        """åˆ†ç±»äº‹ä»¶æ•°æ®å¹¶æå–å®ä½“"""
        events = self._load_events()
        classified_events = defaultdict(list)
        entities = {}
        
        print(f"ğŸ“Š å¼€å§‹åˆ†ç±» {len(events)} ä¸ªäº‹ä»¶...")
        
        for event in events:
            event_type = event.get("event_type", "æœªåˆ†ç±»äº‹ä»¶")
            
            # æ·»åŠ åˆ†ç±»æ ‡ç­¾
            event["layer"] = "events_layer"
            event["suggested_order"] = self._calculate_event_order(event)
            event["comment_prefix"] = "ã€äº‹ä»¶ã€‘"
            
            classified_events[event_type].append(event)
            
            # æå–å®ä½“
            self._extract_entities_from_event(event, entities)
        
        # ä¸ºå®ä½“æ·»åŠ æ ‡ç­¾
        for entity_name, entity_data in entities.items():
            entity_data["layer"] = "entity_layer"
            entity_data["suggested_order"] = self._calculate_entity_order(entity_data)
            entity_data["comment_prefix"] = "ã€æ ¸å¿ƒå®ä½“ã€‘"
        
        # ä¿å­˜åˆ†ç±»ç»“æœ
        self._save_classified_data("classified_events.json", dict(classified_events))
        self._save_classified_data("classified_entities.json", entities)
        
        print(f"âœ… äº‹ä»¶åˆ†ç±»å®Œæˆï¼Œå…± {len(classified_events)} ç§ç±»å‹")
        print(f"âœ… å®ä½“æå–å®Œæˆï¼Œå…± {len(entities)} ä¸ªå®ä½“")
        
        return dict(classified_events), entities
    
    def _detect_rule_type(self, rule: Dict[str, Any]) -> str:
        """æ™ºèƒ½æ£€æµ‹è§„åˆ™ç±»å‹"""
        # 1. ä¼˜å…ˆä½¿ç”¨åŸå§‹æ•°æ®ä¸­çš„rule_typeå­—æ®µ
        if "rule_type" in rule and rule["rule_type"]:
            original_type = rule["rule_type"]
            # æ˜ å°„åŸå§‹ç±»å‹åˆ°æˆ‘ä»¬çš„åˆ†ç±»ä½“ç³»
            type_mapping = {
                "é­”æ³•ä½“ç³»": "é­”æ³•ä½“ç³»",
                "ä¿®ç‚¼ä½“ç³»": "ä¿®ç‚¼ä½“ç³»",
                "ç¤¾ä¼šè§„åˆ™": "ç¤¾ä¼šè§„åˆ™",
                "å†å²èƒŒæ™¯": "å†å²èƒŒæ™¯",
                "åœ°ç†èƒŒæ™¯": "åœ°ç†èƒŒæ™¯",
                "ç¥æ˜è®¾å®š": "ç¥æ˜è®¾å®š",
                "ç‰©ç†æ³•åˆ™": "ç‰©ç†æ³•åˆ™",
                "ç§æ—è®¾å®š": "ç§æ—è®¾å®š",
                "æŠ€æœ¯æ°´å¹³": "æŠ€æœ¯æ°´å¹³",
                "ç»æµä½“ç³»": "ç»æµä½“ç³»",
                "ç”Ÿç‰©è®¾å®š": "ç”Ÿç‰©è®¾å®š"
            }
            if original_type in type_mapping:
                return type_mapping[original_type]

        # 2. å¦‚æœæ²¡æœ‰rule_typeå­—æ®µï¼Œåˆ™åŸºäºå†…å®¹è¿›è¡Œè¯­ä¹‰åˆ†æ
        # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„æè¿°å­—æ®µ
        rule_text = ""
        for field in ["description", "rule_description", "rule_summary"]:
            if field in rule and rule[field]:
                rule_text += rule[field].lower() + " "

        if not rule_text.strip():
            return "å…¶ä»–è§„åˆ™"

        # 3. ä½¿ç”¨å¢å¼ºçš„å…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰ç†è§£
        type_scores = {}
        for rule_type, keywords in self.rule_type_keywords.items():
            score = 0
            # åŸºç¡€å…³é”®è¯åŒ¹é…
            for keyword in keywords:
                if keyword in rule_text:
                    score += 1

            # è¯­ä¹‰ç›¸å…³æ€§åŠ æƒ
            if rule_type == "é­”æ³•ä½“ç³»" and any(word in rule_text for word in ["æ„è¯†åŠ›", "èƒ½åŠ›", "åŠ›é‡", "é­”æ³•", "è¶…èƒ½åŠ›", "å¼‚èƒ½"]):
                score += 2
            elif rule_type == "ä¿®ç‚¼ä½“ç³»" and any(word in rule_text for word in ["è¿›åŒ–", "ä¿®ç‚¼", "æå‡", "æˆé•¿", "çªç ´"]):
                score += 2
            elif rule_type == "ç¤¾ä¼šè§„åˆ™" and any(word in rule_text for word in ["ç»„ç»‡", "åŠ¿åŠ›", "ç¤¾ä¼š", "åˆ¶åº¦", "è§„åˆ™"]):
                score += 2
            elif rule_type == "å†å²èƒŒæ™¯" and any(word in rule_text for word in ["å†å²", "è¿‡å»", "èµ·æº", "èƒŒæ™¯", "æ–‡æ˜"]):
                score += 2
            elif rule_type == "åœ°ç†èƒŒæ™¯" and any(word in rule_text for word in ["åœ°ç†", "ä¸–ç•Œ", "åœ°ç‚¹", "ç©ºé—´", "ç»´åº¦"]):
                score += 2
            elif rule_type == "ç¥æ˜è®¾å®š" and any(word in rule_text for word in ["ç¥", "ç¥æ˜", "ç¥ç¥‡", "å¥³å¨²", "æœ€é«˜ç¥"]):
                score += 2
            elif rule_type == "ç‰©ç†æ³•åˆ™" and any(word in rule_text for word in ["ç‰©ç†", "æ³•åˆ™", "è§„å¾‹", "ç°å®", "æ—¶ç©º"]):
                score += 2
            elif rule_type == "ç»æµä½“ç³»" and any(word in rule_text for word in ["ç»æµ", "è´§å¸", "äº¤æ˜“", "æ™¶çŸ³", "çº¢æ™¶"]):
                score += 2
            elif rule_type == "ç”Ÿç‰©è®¾å®š" and any(word in rule_text for word in ["ç”Ÿç‰©", "æ€ªç‰©", "è¡ç”Ÿç‰©", "ç»†èƒ", "ç—…æ¯’"]):
                score += 2

            if score > 0:
                type_scores[rule_type] = score

        # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å›"å…¶ä»–è§„åˆ™"
        if type_scores:
            return max(type_scores.items(), key=lambda x: x[1])[0]
        else:
            return "å…¶ä»–è§„åˆ™"
    
    def _calculate_rule_order(self, rule_type: str) -> int:
        """è®¡ç®—è§„åˆ™çš„å»ºè®®ä¼˜å…ˆçº§"""
        # è§„åˆ™å±‚ä¼˜å…ˆçº§æ˜ å°„ (0-20)
        priority_map = {
            "å†å²èƒŒæ™¯": 0,
            "ç‰©ç†æ³•åˆ™": 1,
            "é­”æ³•ä½“ç³»": 2,
            "ç¤¾ä¼šè§„åˆ™": 3,
            "åœ°ç†èƒŒæ™¯": 4,
            "ç¥æ˜è®¾å®š": 5,
            "ä¿®ç‚¼ä½“ç³»": 6,
            "ç§æ—è®¾å®š": 7,
            "æŠ€æœ¯æ°´å¹³": 8,
            "ç»æµä½“ç³»": 9,
            "ç”Ÿç‰©è®¾å®š": 10,
            "å…¶ä»–è§„åˆ™": 15
        }
        return priority_map.get(rule_type, 15)
    
    def _calculate_event_order(self, event: Dict[str, Any]) -> int:
        """è®¡ç®—äº‹ä»¶çš„å»ºè®®ä¼˜å…ˆçº§"""
        # åŸºç¡€ä¼˜å…ˆçº§ (60-120)
        base_order = 60
        significance = event.get("significance", 5)
        
        # é‡è¦æ€§è¶Šé«˜ï¼Œä¼˜å…ˆçº§è¶Šé«˜ï¼ˆorderå€¼è¶Šå°ï¼‰
        return base_order + (10 - significance) * 5
    
    def _calculate_entity_order(self, entity: Dict[str, Any]) -> int:
        """è®¡ç®—å®ä½“çš„å»ºè®®ä¼˜å…ˆçº§"""
        # åŸºç¡€ä¼˜å…ˆçº§ (30-50)
        base_order = 30
        event_count = len(entity.get("events", []))
        
        # å‚ä¸äº‹ä»¶è¶Šå¤šï¼Œä¼˜å…ˆçº§è¶Šé«˜
        return base_order + max(0, 20 - event_count)
    
    def _extract_entities_from_event(self, event: Dict[str, Any], entities: Dict[str, Dict[str, Any]]):
        """ä»äº‹ä»¶ä¸­æå–å®ä½“ä¿¡æ¯"""
        participants = event.get("participants", {})
        
        # å¤„ç†ä¸»è¦å‚ä¸è€…
        for participant in participants.get("primary", []):
            if participant and participant.strip():
                if participant not in entities:
                    entities[participant] = {
                        "name": participant,
                        "type": "character",
                        "events": [],
                        "total_significance": 0,
                        "event_count": 0,
                        "locations": [],
                        "items": []
                    }
                
                entities[participant]["events"].append(event)
                entities[participant]["total_significance"] += event.get("significance", 0)
                entities[participant]["event_count"] += 1
                
                # æ·»åŠ ç›¸å…³åœ°ç‚¹å’Œç‰©å“
                location = event.get("location")
                if location and location != "æœªçŸ¥" and location not in entities[participant]["locations"]:
                    entities[participant]["locations"].append(location)

                for item in event.get("key_items", []):
                    if item and item not in entities[participant]["items"]:
                        entities[participant]["items"].append(item)
        
        # å¤„ç†æ¬¡è¦å‚ä¸è€…
        for participant in participants.get("secondary", []):
            if participant and participant.strip():
                if participant not in entities:
                    entities[participant] = {
                        "name": participant,
                        "type": "character",
                        "events": [],
                        "total_significance": 0,
                        "event_count": 0,
                        "locations": [],
                        "items": []
                    }
                
                entities[participant]["events"].append(event)
                entities[participant]["total_significance"] += event.get("significance", 0) * 0.5  # æ¬¡è¦å‚ä¸è€…æƒé‡å‡åŠ
                entities[participant]["event_count"] += 1
        
        # è®¡ç®—å¹³å‡é‡è¦æ€§
        for entity_data in entities.values():
            if entity_data["event_count"] > 0:
                entity_data["average_significance"] = entity_data["total_significance"] / entity_data["event_count"]
            else:
                entity_data["average_significance"] = 0

    def _load_rules(self) -> List[Dict[str, Any]]:
        """åŠ è½½åŸå§‹è§„åˆ™æ•°æ®"""
        rules = []
        rules_dir = self.input_dir / "rules"

        if not rules_dir.exists():
            print(f"âš ï¸ è§„åˆ™ç›®å½•ä¸å­˜åœ¨: {rules_dir}")
            return rules

        # åŠ è½½mapping.jsonè·å–chunké¡ºåº
        mapping_file = Path(self.config.get("output.chunk_dir", "chunks")) / "mapping.json"
        chunk_order = {}

        if mapping_file.exists():
            try:
                with open(mapping_file, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
                chunk_order = {chunk['id']: chunk.get('order', 0) for chunk in mapping.get('chunks', [])}
            except Exception as e:
                print(f"âš ï¸ åŠ è½½mappingæ–‡ä»¶å¤±è´¥: {e}")

        # æŒ‰æ–‡ä»¶åæ’åºåŠ è½½è§„åˆ™æ–‡ä»¶
        rule_files = sorted(rules_dir.glob("chunk_*.json"))

        for file in rule_files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    chunk_rules = json.load(f)

                # ä¸ºæ¯ä¸ªè§„åˆ™æ·»åŠ å…ƒæ•°æ®
                chunk_id = file.stem
                for rule in chunk_rules:
                    if isinstance(rule, dict):
                        rule['source_chunk'] = chunk_id
                        rule['chunk_order'] = chunk_order.get(chunk_id, 0)
                        rules.append(rule)

            except Exception as e:
                print(f"âš ï¸ åŠ è½½è§„åˆ™æ–‡ä»¶ {file.name} å¤±è´¥: {e}")
                continue

        print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(rules)} ä¸ªè§„åˆ™")
        return rules

    def _load_events(self) -> List[Dict[str, Any]]:
        """åŠ è½½åŸå§‹äº‹ä»¶æ•°æ®"""
        events = []
        events_dir = self.input_dir / "events"

        if not events_dir.exists():
            print(f"âš ï¸ äº‹ä»¶ç›®å½•ä¸å­˜åœ¨: {events_dir}")
            return events

        # åŠ è½½mapping.jsonè·å–chunké¡ºåº
        mapping_file = Path(self.config.get("output.chunk_dir", "chunks")) / "mapping.json"
        chunk_order = {}

        if mapping_file.exists():
            try:
                with open(mapping_file, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
                chunk_order = {chunk['id']: chunk.get('order', 0) for chunk in mapping.get('chunks', [])}
            except Exception as e:
                print(f"âš ï¸ åŠ è½½mappingæ–‡ä»¶å¤±è´¥: {e}")

        # æŒ‰æ–‡ä»¶åæ’åºåŠ è½½äº‹ä»¶æ–‡ä»¶
        event_files = sorted(events_dir.glob("chunk_*.json"))

        for file in event_files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    chunk_events = json.load(f)

                # ä¸ºæ¯ä¸ªäº‹ä»¶æ·»åŠ å…ƒæ•°æ®
                chunk_id = file.stem
                for event in chunk_events:
                    if isinstance(event, dict):
                        event['source_chunk'] = chunk_id
                        event['chunk_order'] = chunk_order.get(chunk_id, 0)
                        events.append(event)

            except Exception as e:
                print(f"âš ï¸ åŠ è½½äº‹ä»¶æ–‡ä»¶ {file.name} å¤±è´¥: {e}")
                continue

        print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(events)} ä¸ªäº‹ä»¶")
        return events

    def _save_classified_data(self, filename: str, data: Any):
        """ä¿å­˜åˆ†ç±»åçš„æ•°æ®"""
        output_file = self.output_dir / filename
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… å·²ä¿å­˜åˆ†ç±»æ•°æ®: {output_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†ç±»æ•°æ®å¤±è´¥ {filename}: {e}")

    def _generate_classification_report(self, classified_rules: Dict, classified_events: Dict, entities: Dict):
        """ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡æŠ¥å‘Š"""
        report = {
            "classification_summary": {
                "rules": {
                    "total_count": sum(len(rules) for rules in classified_rules.values()),
                    "types_count": len(classified_rules),
                    "types": {rule_type: len(rules) for rule_type, rules in classified_rules.items()}
                },
                "events": {
                    "total_count": sum(len(events) for events in classified_events.values()),
                    "types_count": len(classified_events),
                    "types": {event_type: len(events) for event_type, events in classified_events.items()}
                },
                "entities": {
                    "total_count": len(entities),
                    "top_entities": sorted(
                        [(name, data["event_count"]) for name, data in entities.items()],
                        key=lambda x: x[1], reverse=True
                    )[:10]
                }
            },
            "layer_distribution": {
                "rules_layer": sum(len(rules) for rules in classified_rules.values()),
                "events_layer": sum(len(events) for events in classified_events.values()),
                "entity_layer": len(entities),
                "timeline_layer": 1  # æ—¶é—´çº¿å±‚å›ºå®šä¸º1ä¸ªæ¡ç›®
            }
        }

        self._save_classified_data("classification_report.json", report)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "="*60)
        print("ğŸ“Š åˆ†ç±»ç»Ÿè®¡æŠ¥å‘Š")
        print("="*60)
        print(f"ğŸ“‹ è§„åˆ™å±‚: {report['layer_distribution']['rules_layer']} ä¸ªæ¡ç›®ï¼Œ{report['classification_summary']['rules']['types_count']} ç§ç±»å‹")
        print(f"ğŸ“… äº‹ä»¶å±‚: {report['layer_distribution']['events_layer']} ä¸ªæ¡ç›®ï¼Œ{report['classification_summary']['events']['types_count']} ç§ç±»å‹")
        print(f"ğŸ‘¥ å®ä½“å±‚: {report['layer_distribution']['entity_layer']} ä¸ªæ¡ç›®")
        print(f"â° æ—¶é—´çº¿å±‚: {report['layer_distribution']['timeline_layer']} ä¸ªæ¡ç›®")
        print("="*60)


if __name__ == "__main__":
    classifier = WorldbookClassifier()
    classifier.classify_all()
