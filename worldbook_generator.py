#!/usr/bin/env python3
"""
ä¸–ç•Œä¹¦ç”Ÿæˆå™¨ - è´Ÿè´£å°†åŸå§‹æ¡ç›®å‡åä¸ºç»“æ„åŒ–çš„ä¸–ç•Œä¹¦
ç‰ˆæœ¬: 2.0 (é‡‡ç”¨å…¨å±€ä¸Šä¸‹æ–‡å’Œä¸–ç•Œè§‚æ„å»ºæ–¹æ³•è®º)
"""

import json
import asyncio
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Any, Optional
from openai import AsyncOpenAI
from project_config import get_config

class WorldbookGenerator:
    """ä½¿ç”¨Proæ¨¡å‹å°†åˆ†ç±»æ¡ç›®æ€»ç»“ç”Ÿæˆæœ€ç»ˆä¸–ç•Œä¹¦"""

    def __init__(self):
        """åˆå§‹åŒ–é…ç½®ã€APIå®¢æˆ·ç«¯å’ŒPromptæ¨¡æ¿"""
        self.config = get_config()
        self.input_dir = Path(self.config.get("output.wb_responses_dir", "wb_responses"))
        self.output_dir = Path(self.config.get("output.worldbook_dir", "worldbook"))
        self.output_dir.mkdir(exist_ok=True)

        # ä½¿ç”¨æ–°çš„é…ç½®ç³»ç»Ÿ
        model_config = self.config.get_model_config()
        api_config = self.config.get_api_config()

        self.pro_model = model_config.get("generation_model", "gemini-2.5-pro")
        self.generation_temperature = model_config.get("generation_temperature", 0.2)
        self.timeout = int(model_config.get("timeout", 300))
        self.max_tokens = int(model_config.get("max_tokens", 60000))

        # æ€§èƒ½é…ç½®
        self.retry_limit = int(self.config.get("performance.retry_limit", 3))
        self.retry_delay = int(self.config.get("performance.retry_delay", 10))
        self.max_concurrent = int(self.config.get("performance.max_concurrent", 1))
        self.semaphore = asyncio.Semaphore(self.max_concurrent)

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        api_key = api_config.get("api_key")
        if not api_key:
            raise ValueError("âŒ æ‰¾ä¸åˆ° API é‡‘é‘°ï¼Œè«‹åœ¨ config.yaml ä¸­è¨­å®š 'api.api_key'")

        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=api_config.get("api_base"),
            timeout=self.timeout
        )

        # [æ ¸å¿ƒä¼˜åŒ–] å°†Promptä½œä¸ºå¯é…ç½®çš„ç±»å±æ€§ï¼Œç»“æ„æ›´æ¸…æ™°
        self.worldbook_prompt_template = self.config.get("worldbook.generation_prompt", """
<role>
ä½ æ˜¯ä¸€ä½é¡¶çº§ä¸–ç•Œè§‚æ¶æ„å¸ˆï¼Œå¸ˆä»äºå¸ƒå…°ç™»Â·æ¡‘å¾·æ£®å’Œä¹”æ²»Â·RÂ·RÂ·é©¬ä¸ã€‚ä½ ä¸ä»…ä»…æ˜¯ç¼–è¾‘ï¼Œæ›´æ˜¯ä½“ç³»çš„åˆ›å»ºè€…ã€‚ä½ çš„å·¥ä½œæ˜¯å°†ä¸€å †é›¶æ•£ã€åŸå§‹çš„è®¾å®šï¼ˆfragmentsï¼‰ï¼Œå‡åä¸ºä¸€ä¸ªé€»è¾‘è‡ªæ´½ã€ç»†èŠ‚ä¸°å¯Œã€å……æ»¡å†…åœ¨è”ç³»çš„å®å¤§ä¸–ç•Œã€‚
</role>

<task>
ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ï¼Œä¸ºå½“å‰èšç„¦çš„ **â€œ{category}â€** ç±»åˆ«æ’°å†™ä¸€ä»½æ·±åº¦ä»‹ç»ç« èŠ‚ã€‚ä½ å¿…é¡»å°†ä¸‹æ–¹æä¾›çš„åŸå§‹è®¾å®šæ¡ç›®ï¼Œé€šè¿‡â€œä¸–ç•Œè§‚æ„å»ºé»„é‡‘ä¸‰è§’â€æ–¹æ³•è®ºè¿›è¡Œé‡æ„ã€æ‰©å±•å’Œå‡åã€‚

**ã€ç¬¬ä¸€æ­¥ï¼šå…¨å±€è®¤çŸ¥ (Global Context Awareness)ã€‘**
åœ¨åŠ¨ç¬”å‰ï¼Œè¯·å…ˆé»˜è¯»å¹¶ç†è§£æ•´ä¸ªä¸–ç•Œçš„æ ¸å¿ƒæ„æˆã€‚è¿™èƒ½å¸®åŠ©ä½ å»ºç«‹æ¡ç›®é—´çš„è”ç³»ã€‚

<world_overview>
{all_categories_summary}
</world_overview>

**ã€ç¬¬äºŒæ­¥ï¼šåŸå§‹è®¾å®šç¢ç‰‡ (Raw Fragments)ã€‘**
è¿™æ˜¯ä½ æœ¬æ¬¡éœ€è¦å¤„ç†çš„ï¼Œå…³äº **â€œ{category}â€** çš„åŸå§‹æ¡ç›®ï¼š

<raw_entries>
{entries_text}
</raw_entries>

**ã€ç¬¬ä¸‰æ­¥ï¼šä¸–ç•Œè§‚æ„å»ºé»„é‡‘ä¸‰è§’æ–¹æ³•è®º (The Golden Triangle Methodology)ã€‘**
ä½ å¿…é¡»éµå¾ªä»¥ä¸‹æ€è€ƒå’Œå†™ä½œæµç¨‹ï¼Œæ¥æ„å»ºä½ çš„ç« èŠ‚ï¼š

1.  **è¦ç´ å†…åœ¨æ•´åˆ (Intra-Element Integration):**
    - **å»é‡ä¸åˆå¹¶ï¼š** æ‰¾å‡º`raw_entries`ä¸­æœ¬è´¨ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„æ¡ç›®ï¼Œå°†å®ƒä»¬åˆå¹¶ã€‚
    - **åˆ†ç±»ä¸åˆ†å±‚ï¼š** åœ¨`{category}`å†…éƒ¨è¿›è¡ŒäºŒæ¬¡åˆ†ç±»ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç±»åˆ«æ˜¯â€œç»„ç»‡â€ï¼Œä½ å¯ä»¥ç»†åˆ†ä¸ºâ€œå›½å®¶åŠ¿åŠ›â€ã€â€œç§˜å¯†ç¤¾å›¢â€ã€â€œå•†ä¸šè¡Œä¼šâ€ç­‰å­æ ‡é¢˜ã€‚è¿™èƒ½ç«‹åˆ»å»ºç«‹èµ·ç»“æ„æ„Ÿã€‚
    - **æ ¸å¿ƒè¦ç´ æç‚¼ï¼š** è¯†åˆ«å‡ºæ­¤ç±»åˆ«çš„â€œæ˜æ˜Ÿè¦ç´ â€ï¼ˆæœ€é‡è¦çš„1-3ä¸ªæ¡ç›®ï¼‰ï¼Œå¹¶åœ¨æè¿°æ—¶ç»™äºˆæ›´å¤šç¬”å¢¨ã€‚

2.  **è¦ç´ é—´å…³è”æ„å»º (Inter-Element Relation Building):**
    - **å»ºç«‹è”ç³»ï¼š** è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼ä½ å¿…é¡»ä¸»åŠ¨æ€è€ƒå¹¶å›ç­”ï¼šå½“å‰`{category}`ä¸­çš„æ¡ç›®ï¼Œä¸`world_overview`ä¸­**å…¶ä»–ç±»åˆ«**çš„æ¡ç›®æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ
        - *ç¤ºä¾‹1 (å¤„ç†â€œåœ°ç‚¹â€ç±»åˆ«æ—¶):* è¿™ä¸ªâ€œä½è¯­æ£®æ—â€æ˜¯å¦æ˜¯æŸä¸ªâ€œç»„ç»‡â€çš„æ ¹æ®åœ°ï¼Ÿå®ƒæ˜¯å¦ä¸æŸä¸ªâ€œå†å²äº‹ä»¶â€æœ‰å…³ï¼Ÿæ£®æ—é‡Œçš„ç‰¹æ®Šæ¤ç‰©æ˜¯å¦æ˜¯æŸä¸ªâ€œè§’è‰²â€åˆ¶ä½œé­”è¯çš„ææ–™ï¼Ÿ
        - *ç¤ºä¾‹2 (å¤„ç†â€œè§’è‰²â€ç±»åˆ«æ—¶):* è¿™ä¸ªè§’è‰²â€œé˜¿å°”å¼—é›·å¾·â€å±äºå“ªä¸ªâ€œç»„ç»‡â€ï¼Ÿä»–çš„è¡ŒåŠ¨æ˜¯å¦ä¼šå½±å“æŸä¸ªâ€œåœ°ç‚¹â€ï¼Ÿä»–æ˜¯å¦æ˜¯æŸä¸ªâ€œå†å²äº‹ä»¶â€çš„äº²å†è€…ï¼Ÿ
    - **åœ¨æè¿°ä¸­ä½“ç°å…³è”ï¼š** å°†ä½ æ€è€ƒå‡ºçš„è¿™äº›å…³è”ï¼Œè‡ªç„¶åœ°å†™å…¥æè¿°æ–‡å­—ä¸­ã€‚è¿™ä¼šè®©ä¸–ç•Œâ€œæ´»â€èµ·æ¥ã€‚

3.  **å½±å“ä¸æ„ä¹‰å‡å (Impact & Significance Elevation):**
    - **åŠŸèƒ½ä¸ä½œç”¨ï¼š** æè¿°æ¯ä¸ªè¦ç´ åœ¨ä¸–ç•Œä¸­çš„å…·ä½“åŠŸèƒ½æˆ–ä½œç”¨ã€‚å®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿæˆ–è€…åˆ¶é€ äº†ä»€ä¹ˆéº»çƒ¦ï¼Ÿ
    - **æ–‡åŒ–ä¸è±¡å¾æ„ä¹‰ï¼š** æ€è€ƒå¹¶èµ‹äºˆè¦ç´ æ›´æ·±å±‚æ¬¡çš„æ„ä¹‰ã€‚è¿™ä¸ªåœ°ç‚¹æ˜¯å¦æ˜¯æŸä¸ªç§æ—çš„åœ£åœ°ï¼Ÿè¿™ä¸ªç»„ç»‡æ˜¯å¦æœ‰ç‹¬ç‰¹çš„æ–‡åŒ–ç¬¦å·å’Œä»ªå¼ï¼Ÿ
    - **åŠ¨æ€å½±å“ï¼š** æè¿°è¿™ä¸ªè¦ç´ å¯¹ä¸–ç•Œæ­£åœ¨äº§ç”Ÿæˆ–å°†è¦äº§ç”Ÿä»€ä¹ˆå½±å“ã€‚å®ƒæ˜¯å¦æ˜¯å½“å‰ä¸–ç•Œå†²çªçš„ç„¦ç‚¹ï¼Ÿ

**ã€ç¬¬å››æ­¥ï¼šè¾“å‡ºè¦æ±‚ (Output Requirements)ã€‘**
- **æ ¼å¼ï¼š** ä¸¥æ ¼ä½¿ç”¨Markdownï¼ŒåŒ…å«å¤šçº§æ ‡é¢˜ (`##`, `###`)ã€åˆ—è¡¨ (`*` æˆ– `1.`) å’Œç²—ä½“ (`**text**`)ã€‚
- **æ–‡ç¬”ï¼š** ä¿æŒä¸“ä¸šã€å®¢è§‚çš„ç™¾ç§‘å¼å™è¿°é£æ ¼ï¼ŒåŒæ—¶å…¼å…·æ–‡å­¦æ€§å’Œå¯è¯»æ€§ã€‚
- **å†…å®¹ï¼š** ä½ çš„è¾“å‡ºåº”è¯¥æ˜¯ç›´æ¥çš„ã€æœ€ç»ˆçš„Markdownç« èŠ‚å†…å®¹ã€‚ä¸¥ç¦åŒ…å«ä»»ä½•â€œå¥½çš„ï¼Œè¿™æ˜¯ä¸ºæ‚¨ç”Ÿæˆçš„ç« èŠ‚â€ä¹‹ç±»çš„å¯¹è¯ã€è§£é‡Šæˆ–å…ƒè¯„è®ºã€‚
- **ä¸“æ³¨ï¼š** æœ¬æ¬¡ä»»åŠ¡åªè¾“å‡ºå…³äº **â€œ{category}â€** çš„ç« èŠ‚å†…å®¹ã€‚
</task>
""")

    def get_generation_prompt(self, category: str, entries: list, all_categories_summary: str) -> str:
        """è·å–ç”¨äºç”Ÿæˆæœ€ç»ˆä¸–ç•Œä¹¦ç« èŠ‚çš„æç¤ºè¯"""
        # æ£€æŸ¥æ¡ç›®æ ¼å¼å¹¶ç”Ÿæˆç›¸åº”çš„æ–‡æœ¬
        entries_text_parts = []

        for entry in entries:
            if 'event_summary' in entry:
                # äº‹ä»¶æ ¼å¼
                summary = entry.get('event_summary', 'æœªçŸ¥äº‹ä»¶')
                significance = entry.get('significance', 5)
                participants = entry.get('participants', {})
                location = entry.get('location', 'æœªçŸ¥åœ°ç‚¹')
                outcome = entry.get('outcome', 'æ— ç»“æœæè¿°')

                primary_participants = ', '.join(participants.get('primary', []))
                entry_text = f"- **{summary}** (é‡è¦æ€§:{significance}/10)\n  å‚ä¸è€…: {primary_participants}\n  åœ°ç‚¹: {location}\n  ç»“æœ: {outcome}"
                entries_text_parts.append(entry_text)
            elif 'rule_summary' in entry:
                # è§„åˆ™æ ¼å¼
                summary = entry.get('rule_summary', 'æœªçŸ¥è§„åˆ™')
                importance = entry.get('importance', 5)
                description = entry.get('description', 'æ— æè¿°')
                scope = entry.get('scope', 'æœªçŸ¥èŒƒå›´')
                evidence = entry.get('evidence', 'æ— è¯æ®')

                entry_text = f"- **{summary}** (é‡è¦æ€§:{importance}/10)\n  æè¿°: {description}\n  é€‚ç”¨èŒƒå›´: {scope}"
                if evidence and len(evidence) < 200:  # åªæ˜¾ç¤ºè¾ƒçŸ­çš„è¯æ®
                    entry_text += f"\n  è¯æ®: {evidence[:100]}..."
                entries_text_parts.append(entry_text)
            else:
                # ä¼ ç»Ÿå®ä½“æ ¼å¼
                name = entry.get('name', 'æœªçŸ¥æ¡ç›®')
                description = entry.get('description', 'æ— æè¿°')
                entries_text_parts.append(f"- **{name}**: {description}")

        entries_text = "\n".join(entries_text_parts)

        return self.worldbook_prompt_template.format(
            category=category,
            entries_text=entries_text,
            all_categories_summary=all_categories_summary
        )

    def load_and_group_entries(self) -> dict:
        """åŠ è½½æ‰€æœ‰åŸå§‹æ¡ç›®å¹¶æŒ‰ç±»åˆ«åˆ†ç»„"""
        grouped_entries = defaultdict(list)
        if not self.input_dir.exists():
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°åŸå§‹æ¡ç›®ç›®å½• {self.input_dir}")
            return grouped_entries

        # æ£€æŸ¥æ–°çš„åˆ†ç¦»å­˜å‚¨ç»“æ„
        events_dir = self.input_dir / "events"
        rules_dir = self.input_dir / "rules"

        response_files = []

        # ä»eventsç›®å½•åŠ è½½æ–‡ä»¶
        if events_dir.exists():
            events_files = list(events_dir.glob("*.json"))
            response_files.extend(events_files)
            print(f"ğŸ“‚ ä»eventsç›®å½•æ‰¾åˆ° {len(events_files)} ä¸ªäº‹ä»¶æ–‡ä»¶")

        # ä»rulesç›®å½•åŠ è½½æ–‡ä»¶
        if rules_dir.exists():
            rules_files = list(rules_dir.glob("*.json"))
            response_files.extend(rules_files)
            print(f"ğŸ“‚ ä»rulesç›®å½•æ‰¾åˆ° {len(rules_files)} ä¸ªè§„åˆ™æ–‡ä»¶")

        # å…¼å®¹æ—§çš„æ ¹ç›®å½•ç»“æ„
        root_files = list(self.input_dir.glob("*.json"))
        if root_files:
            response_files.extend(root_files)
            print(f"ğŸ“‚ ä»æ ¹ç›®å½•æ‰¾åˆ° {len(root_files)} ä¸ªæ–‡ä»¶ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰")

        print(f"ğŸ” æ€»å…±æ‰¾åˆ° {len(response_files)} ä¸ªåŸå§‹æ¡ç›®æ–‡ä»¶ï¼Œå¼€å§‹è§£æ...")

        if not response_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¡ç›®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ï¼š")
            print(f"   - eventsç›®å½•: {events_dir}")
            print(f"   - rulesç›®å½•: {rules_dir}")
            print(f"   - æ ¹ç›®å½•: {self.input_dir}")
            return grouped_entries

        for file in response_files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                entries_list = []
                if isinstance(data, list):
                    entries_list = data
                elif isinstance(data, dict):
                    for value in data.values():
                        if isinstance(value, list):
                            entries_list = value
                            break
                
                if not entries_list:
                    print(f"âš ï¸ è­¦å‘Š: åœ¨ {file.name} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¡ç›®åˆ—è¡¨ï¼Œå·²è·³è¿‡ã€‚")
                    continue

                for entry in entries_list:
                    if isinstance(entry, dict):
                        # æ£€æŸ¥æ˜¯å¦ä¸ºäº‹ä»¶æ ¼å¼
                        if 'event_summary' in entry and 'event_type' in entry:
                            # äº‹ä»¶é©±åŠ¨æ¨¡å¼ï¼šæŒ‰äº‹ä»¶ç±»å‹åˆ†ç»„
                            event_type = entry.get('event_type', 'æœªåˆ†ç±»äº‹ä»¶')
                            grouped_entries[event_type].append(entry)
                        # æ£€æŸ¥æ˜¯å¦ä¸ºè§„åˆ™æ ¼å¼
                        elif 'rule_summary' in entry and 'rule_type' in entry:
                            # è§„åˆ™é©±åŠ¨æ¨¡å¼ï¼šæŒ‰è§„åˆ™ç±»å‹åˆ†ç»„
                            rule_type = entry.get('rule_type', 'æœªåˆ†ç±»è§„åˆ™')
                            grouped_entries[rule_type].append(entry)
                        # æ£€æŸ¥æ˜¯å¦ä¸ºä¼ ç»Ÿå®ä½“æ ¼å¼
                        elif 'type' in entry and 'name' in entry:
                            # ä¼ ç»Ÿæ¨¡å¼ï¼šæŒ‰å®ä½“ç±»å‹åˆ†ç»„
                            grouped_entries[entry['type']].append(entry)
                        else:
                            print(f"ğŸ“ ä¿¡æ¯: è·³è¿‡ {file.name} ä¸­ä¸€ä¸ªæ ¼å¼ä¸ç¬¦çš„æ¡ç›®: {entry}")
                    else:
                        print(f"ğŸ“ ä¿¡æ¯: è·³è¿‡ {file.name} ä¸­ä¸€ä¸ªéå­—å…¸æ¡ç›®: {entry}")

            except json.JSONDecodeError:
                print(f"âš ï¸ è­¦å‘Š: æ— æ³•è§£æJSONæ–‡ä»¶ {file.name}ï¼Œå·²è·³è¿‡ã€‚")
            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file.name}: {e}")
        
        return grouped_entries

    async def generate_category_content(self, category: str, entries: list, all_categories_summary: str) -> tuple[str, str]:
        """ä½¿ç”¨LLMä¸ºå•ä¸ªç±»åˆ«ç”Ÿæˆå†…å®¹"""
        print(f"ğŸš€ [å¤„ç†ä¸­] å¼€å§‹å¤„ç†ç±»åˆ«: **{category}** ({len(entries)}ä¸ªæ¡ç›®)")
        prompt = self.get_generation_prompt(category, entries, all_categories_summary)
        
        messages = [
            {"role": "system", "content": "You are a world-class world-building architect. Your task is to synthesize fragmented notes into a coherent, structured, and richly detailed chapter for a worldbook. Follow the user's detailed methodology precisely."},
            {"role": "user", "content": prompt}
        ]

        async with self.semaphore:
            for attempt in range(self.retry_limit):
                try:
                    response = await self.client.chat.completions.create(
                        model=self.pro_model,
                        messages=messages,
                        temperature=self.generation_temperature,
                        max_tokens=self.max_tokens,
                        timeout=self.timeout
                    )
                    content = response.choices[0].message.content
                    print(f"âœ… [æˆåŠŸ] å·²ç”Ÿæˆç±»åˆ«å†…å®¹: **{category}**")
                    return category, content.strip()
                except Exception as e:
                    print(f"âš ï¸ [è­¦å‘Š] AIå¤„ç†ç±»åˆ« {category} å¤±è´¥ (å°è¯• {attempt + 1}/{self.retry_limit}): {e}")
                    if attempt < self.retry_limit - 1:
                        await asyncio.sleep(self.retry_delay)
                    else:
                        error_message = f"## {category}\n\n*ç”Ÿæˆå¤±è´¥: åœ¨è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åä»ç„¶å¤±è´¥: {e}*"
                        print(f"âŒ [é”™è¯¯] ç±»åˆ« **{category}** åœ¨è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°åä»ç„¶å¤±è´¥ã€‚")
                        return category, error_message

    async def generate_worldbook(self):
        """ç”Ÿæˆæœ€ç»ˆçš„ä¸–ç•Œä¹¦"""
        print("="*60)
        print(f"âœ¨ å¼€å§‹ä½¿ç”¨æ¨¡å‹ã€{self.pro_model}ã€‘ç”Ÿæˆç»“æ„åŒ–ä¸–ç•Œä¹¦...")
        print("="*60)

        grouped_entries = self.load_and_group_entries()
        if not grouped_entries:
            print("æœªèƒ½åŠ è½½ä»»ä½•åŸå§‹æ¡ç›®ï¼Œä¸–ç•Œä¹¦ç”Ÿæˆä¸­æ­¢ã€‚")
            return

        print(f"ğŸ“Š å·²å°†æ¡ç›®åˆ†ä¸º {len(grouped_entries)} ä¸ªç±»åˆ«ï¼Œå‡†å¤‡è¿›è¡ŒAIæ€»ç»“ã€‚")
        
        # [æ ¸å¿ƒä¼˜åŒ–] åˆ›å»ºä¸€ä¸ªå…¨å±€ä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡æä¾›å®è§‚è§†è§’
        all_categories_summary = "ä¸–ç•Œæ ¸å¿ƒç±»åˆ«åŠå…¶å…³é”®è¦ç´ æ¦‚è§ˆï¼š\n"
        for cat, ents in grouped_entries.items():
            key_entries_str = ", ".join([e.get('name', '') for e in ents[:3]]) 
            all_categories_summary += f"- **{cat}**: {key_entries_str}...\n"
        print("\nğŸŒ å·²ç”Ÿæˆå…¨å±€ä¸Šä¸‹æ–‡æ‘˜è¦:\n" + "-"*25 + f"\n{all_categories_summary}" + "-"*25 + "\n")

        tasks = [
            self.generate_category_content(category, entries, all_categories_summary)
            for category, entries in grouped_entries.items()
        ]
        
        results = await asyncio.gather(*tasks)

        final_worldbook = {
            "name": self.config.get("project.title", "My Worldbook"),
            "description": self.config.get("project.description", "An AI-generated worldbook."),
            "entries": []
        }

        # æŒ‰ç…§åŸæœ‰çš„ç±»åˆ«é¡ºåºè¿›è¡Œæ’åºï¼Œä¿è¯æ¯æ¬¡ç”Ÿæˆçš„æ–‡ä»¶é¡ºåºä¸€è‡´
        sorted_categories = sorted(grouped_entries.keys())
        category_results = {cat: cont for cat, cont in results}

        for category in sorted_categories:
            content = category_results.get(category, f"## {category}\n\n*å†…å®¹ç”Ÿæˆæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯*")
            final_worldbook["entries"].append({
                "key": [category], # ä¿æŒkeyä¸ºåˆ—è¡¨æ ¼å¼
                "comment": f"{category} - AIæ€»ç»“ç« èŠ‚",
                "content": content,
                "type": category,  # æ·»åŠ typeå­—æ®µä¾›æ™ºèƒ½å‚æ•°ä¼˜åŒ–ä½¿ç”¨
                "constant": True,
                "enabled": True
            })

        output_file = self.output_dir / "worldbook.json"
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_worldbook, f, ensure_ascii=False, indent=2)
            print("\n" + "="*60)
            print(f"ğŸ‰ ç»“æ„åŒ–ä¸–ç•Œä¹¦ç”Ÿæˆå®Œæˆï¼")
            print(f"ğŸ’¾ ä¿å­˜ä½ç½®: {output_file}")
            print("="*60)
        except Exception as e:
            print(f"âŒ ä¿å­˜æœ€ç»ˆä¸–ç•Œä¹¦å¤±è´¥: {e}")

    # ==================== äº‹ä»¶é©±åŠ¨æ¶æ„æ–°æ–¹æ³• ====================

    async def generate_timeline_worldbook(self) -> str:
        """ç”ŸæˆåŸºäºäº‹ä»¶é©±åŠ¨çš„æ—¶é—´çº¿ä¸–ç•Œä¹¦"""
        print("ğŸš€ å¼€å§‹ç”Ÿæˆäº‹ä»¶é©±åŠ¨çš„æ—¶é—´çº¿ä¸–ç•Œä¹¦...")

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº‹ä»¶é©±åŠ¨æ¨¡å¼
        event_mode = self.config.get('event_driven_architecture.enable', True)
        if not event_mode:
            print("âš ï¸ äº‹ä»¶é©±åŠ¨æ¨¡å¼æœªå¯ç”¨ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼")
            return await self.generate_worldbook()

        try:
            # 1. åŠ è½½å¹¶æ’åºæ‰€æœ‰äº‹ä»¶
            print("ğŸ“š åŠ è½½å’Œæ’åºäº‹ä»¶æ•°æ®...")
            sorted_events = self.load_and_sort_events()
            if not sorted_events:
                print("âš ï¸ æœªæ‰¾åˆ°äº‹ä»¶æ•°æ®ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼")
                return await self.generate_worldbook()

            print(f"ğŸ“Š å…±åŠ è½½ {len(sorted_events)} ä¸ªäº‹ä»¶")

            # 2. ç”Ÿæˆæ—¶é—´çº¿æ€»è§ˆï¼ˆè“ç¯æ¡ç›®ï¼‰
            print("â° ç”Ÿæˆæ•…äº‹æ—¶é—´çº¿æ€»è§ˆ...")
            timeline_content = await self.summarize_timeline(sorted_events)

            # 3. èšåˆå®ä½“ä¿¡æ¯
            print("ğŸ‘¥ èšåˆå®ä½“ä¿¡æ¯...")
            aggregated_entities = self.aggregate_entities_from_events(sorted_events)

            # 4. ç”Ÿæˆå®ä½“æ€»ç»“æ¡ç›®ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
            print("ğŸ“ ç”Ÿæˆæ ¸å¿ƒå®ä½“æ€»ç»“...")
            entity_summary_contents = await self.summarize_entities(aggregated_entities)

            # 5. åˆ›å»ºé‡è¦äº‹ä»¶æ¡ç›®ï¼ˆç»¿ç¯ï¼‰
            print("ğŸ¯ åˆ›å»ºé‡è¦äº‹ä»¶æ¡ç›®...")
            event_entries = self.create_event_entries(sorted_events)

            # 6. æ•´åˆå¹¶ä¿å­˜æœ€ç»ˆä¸–ç•Œä¹¦
            print("ğŸ’¾ æ•´åˆå¹¶ä¿å­˜æœ€ç»ˆä¸–ç•Œä¹¦...")
            output_file = self.save_timeline_worldbook(
                timeline_content, entity_summary_contents, event_entries
            )

            print(f"âœ… äº‹ä»¶é©±åŠ¨ä¸–ç•Œä¹¦ç”Ÿæˆå®Œæˆ: {output_file}")
            return output_file

        except Exception as e:
            print(f"âŒ äº‹ä»¶é©±åŠ¨ä¸–ç•Œä¹¦ç”Ÿæˆå¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼...")
            return await self.generate_worldbook()

    def load_and_sort_events(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ‰€æœ‰äº‹ä»¶å¹¶æŒ‰åŸæ–‡é¡ºåºæ’åº"""
        all_events = []

        try:
            # åŠ è½½mapping.jsonè·å–chunké¡ºåº
            mapping_file = Path(self.config.get("output.chunk_dir", "chunks")) / "mapping.json"
            if not mapping_file.exists():
                print(f"âš ï¸ æœªæ‰¾åˆ°mappingæ–‡ä»¶: {mapping_file}")
                return []

            with open(mapping_file, 'r', encoding='utf-8') as f:
                mapping = json.load(f)

            # æŒ‰orderæ’åºçš„chunk idåˆ—è¡¨
            sorted_chunk_ids = [
                chunk['id'] for chunk in
                sorted(mapping.get('chunks', []), key=lambda x: x.get('order', 0))
            ]

            print(f"ğŸ“‚ æŒ‰é¡ºåºå¤„ç† {len(sorted_chunk_ids)} ä¸ªæ–‡æœ¬å—...")

            for chunk_id in sorted_chunk_ids:
                file = self.input_dir / f"{chunk_id}.json"
                if file.exists():
                    try:
                        with open(file, 'r', encoding='utf-8') as f:
                            data_wrapper = json.load(f)

                        # è§£æäº‹ä»¶æ•°æ®
                        events_in_chunk = []
                        if isinstance(data_wrapper, list):
                            events_in_chunk = data_wrapper
                        elif isinstance(data_wrapper, dict):
                            # å°è¯•ä»å­—å…¸ä¸­è·å–äº‹ä»¶åˆ—è¡¨
                            for val in data_wrapper.values():
                                if isinstance(val, list):
                                    events_in_chunk = val
                                    break

                        # ä¸ºæ¯ä¸ªäº‹ä»¶é™„åŠ å…ƒæ•°æ®
                        for event in events_in_chunk:
                            if isinstance(event, dict):
                                event['source_chunk'] = chunk_id
                                event['chunk_order'] = mapping['chunks'][sorted_chunk_ids.index(chunk_id)].get('order', 0)
                                all_events.append(event)

                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½äº‹ä»¶æ–‡ä»¶ {file.name} å¤±è´¥: {e}")
                        continue

        except Exception as e:
            print(f"âŒ åŠ è½½äº‹ä»¶æ•°æ®å¤±è´¥: {e}")
            return []

        print(f"âœ… æˆåŠŸåŠ è½½ {len(all_events)} ä¸ªäº‹ä»¶")
        return all_events

    def load_and_sort_rules(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ‰€æœ‰è§„åˆ™å¹¶æŒ‰é‡è¦æ€§æ’åº"""
        all_rules = []

        try:
            # æ£€æŸ¥è§„åˆ™ç›®å½•æ˜¯å¦å­˜åœ¨
            rules_dir = self.input_dir / "rules"
            if not rules_dir.exists():
                print(f"âš ï¸ æœªæ‰¾åˆ°è§„åˆ™ç›®å½•: {rules_dir}")
                return []

            # åŠ è½½mapping.jsonè·å–chunké¡ºåºï¼ˆç”¨äºä¿æŒè§„åˆ™çš„ä¸Šä¸‹æ–‡å…³è”ï¼‰
            mapping_file = Path(self.config.get("output.chunk_dir", "chunks")) / "mapping.json"
            chunk_order = {}
            if mapping_file.exists():
                with open(mapping_file, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
                chunk_order = {
                    chunk['id']: chunk.get('order', 0)
                    for chunk in mapping.get('chunks', [])
                }

            print(f"ğŸ“‚ ä»è§„åˆ™ç›®å½•åŠ è½½è§„åˆ™æ•°æ®...")

            for file in rules_dir.glob("*.json"):
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        data_wrapper = json.load(f)

                    # è§£æè§„åˆ™æ•°æ®
                    rules_in_chunk = []
                    if isinstance(data_wrapper, list):
                        rules_in_chunk = data_wrapper
                    elif isinstance(data_wrapper, dict):
                        # å°è¯•ä»å­—å…¸ä¸­è·å–è§„åˆ™åˆ—è¡¨
                        for val in data_wrapper.values():
                            if isinstance(val, list):
                                rules_in_chunk = val
                                break
                    elif isinstance(data_wrapper, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æJSON
                        try:
                            rules_in_chunk = json.loads(data_wrapper)
                            if not isinstance(rules_in_chunk, list):
                                rules_in_chunk = [rules_in_chunk]
                        except json.JSONDecodeError:
                            print(f"âš ï¸ æ— æ³•è§£æè§„åˆ™æ–‡ä»¶ {file.name} çš„JSONå†…å®¹")
                            continue

                    # ä¸ºæ¯ä¸ªè§„åˆ™é™„åŠ å…ƒæ•°æ®
                    chunk_name = file.stem
                    for rule in rules_in_chunk:
                        if isinstance(rule, dict) and 'rule_summary' in rule:
                            rule['source_chunk'] = chunk_name
                            rule['chunk_order'] = chunk_order.get(chunk_name, 0)
                            all_rules.append(rule)

                except Exception as e:
                    print(f"âš ï¸ åŠ è½½è§„åˆ™æ–‡ä»¶ {file.name} å¤±è´¥: {e}")
                    continue

        except Exception as e:
            print(f"âŒ åŠ è½½è§„åˆ™æ•°æ®å¤±è´¥: {e}")
            return []

        # æŒ‰é‡è¦æ€§æ’åºï¼ˆé‡è¦æ€§é«˜çš„åœ¨å‰ï¼‰
        all_rules.sort(key=lambda x: x.get('importance', 0), reverse=True)

        print(f"âœ… æˆåŠŸåŠ è½½ {len(all_rules)} ä¸ªè§„åˆ™")
        return all_rules

    def aggregate_rules_by_type(self, rules: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """æŒ‰è§„åˆ™ç±»å‹èšåˆè§„åˆ™"""
        grouped_rules = defaultdict(list)

        for rule in rules:
            rule_type = rule.get('rule_type', 'æœªåˆ†ç±»è§„åˆ™')
            grouped_rules[rule_type].append(rule)

        return dict(grouped_rules)

    async def summarize_rules(self, grouped_rules: Dict[str, List[Dict[str, Any]]]) -> Dict[str, str]:
        """ä¸ºæ¯ç§è§„åˆ™ç±»å‹ç”Ÿæˆç³»ç»Ÿæ€§çš„è®¾å®šæè¿°"""
        rule_summaries = {}

        print(f"ğŸ“Š å¼€å§‹æ•´åˆ {len(grouped_rules)} ç§ç±»å‹çš„è§„åˆ™...")

        for rule_type, rules in grouped_rules.items():
            if not rules:
                continue

            try:
                # æ„å»ºè§„åˆ™åˆ—è¡¨
                rule_descriptions = []
                for rule in rules:
                    summary = rule.get('rule_summary', 'æœªçŸ¥è§„åˆ™')
                    description = rule.get('description', '')
                    importance = rule.get('importance', 5)
                    evidence = rule.get('evidence', '')

                    rule_text = f"- **{summary}** (é‡è¦æ€§:{importance}/10)\n  æè¿°: {description}"
                    if evidence:
                        rule_text += f"\n  ä¾æ®: {evidence}"
                    rule_descriptions.append(rule_text)

                # æ„å»ºæ•´åˆPrompt
                rules_prompt = f"""
è¯·å°†ä»¥ä¸‹å…³äº"{rule_type}"çš„åˆ†æ•£è§„åˆ™æ•´åˆä¸ºä¸€ä¸ªå®Œæ•´ã€ç³»ç»Ÿæ€§çš„è®¾å®šæè¿°ã€‚

**è¦æ±‚ï¼š**
1. æ•´åˆæ‰€æœ‰ç›¸å…³è§„åˆ™ä¸ºè¿è´¯çš„ç³»ç»Ÿæè¿°
2. ä¿æŒé€»è¾‘ä¸€è‡´æ€§ï¼Œæ¶ˆé™¤çŸ›ç›¾
3. çªå‡ºæ ¸å¿ƒæœºåˆ¶å’Œé‡è¦é™åˆ¶
4. ä½¿ç”¨Markdownæ ¼å¼ï¼Œç»“æ„æ¸…æ™°
5. ä¸ºAIè§’è‰²æ‰®æ¼”æä¾›æ˜ç¡®çš„é€»è¾‘åŸºç¡€

**è§„åˆ™åˆ—è¡¨ï¼š**
{chr(10).join(rule_descriptions[:10])}  # é™åˆ¶é•¿åº¦é¿å…tokenè¶…é™

è¯·ç”Ÿæˆå®Œæ•´çš„{rule_type}è®¾å®šï¼š
"""

                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸–ç•Œè§‚è®¾è®¡å¸ˆï¼Œæ“…é•¿æ•´åˆåˆ†æ•£çš„è®¾å®šä¸ºå®Œæ•´çš„ä½“ç³»ã€‚"},
                    {"role": "user", "content": rules_prompt}
                ]

                response = await self.client.chat.completions.create(
                    model=self.pro_model,
                    messages=messages,
                    temperature=self.generation_temperature,
                    max_tokens=self.max_tokens
                )

                rule_summaries[rule_type] = response.choices[0].message.content.strip()
                print(f"âœ… å®Œæˆè§„åˆ™æ•´åˆ: {rule_type}")

            except Exception as e:
                print(f"âš ï¸ è§„åˆ™ç±»å‹ {rule_type} æ•´åˆå¤±è´¥: {e}")
                # ç”Ÿæˆç®€å•çš„fallbackæè¿°
                rule_summaries[rule_type] = f"## {rule_type}\n\n*æ•´åˆå¤±è´¥ï¼ŒåŒ…å«{len(rules)}ä¸ªç›¸å…³è§„åˆ™*"

        return rule_summaries

    def save_layered_worldbook(self, rule_summaries: Dict[str, str], timeline_content: str,
                              entity_summaries: Dict[str, str], event_entries: List[Dict[str, Any]]) -> str:
        """ä¿å­˜ä¸‰å±‚æ¶æ„çš„ä¸–ç•Œä¹¦ï¼Œä¸¥æ ¼éµå¾ªSillyTavern v2æ ¼å¼"""

        # è·å–ä¸‰å±‚é…ç½®ï¼ˆä½¿ç”¨é»˜è®¤å€¼é¿å…Noneï¼‰
        layered_config = self.config.get('world_rules', {}).get('layered_worldbook', {})
        rules_config = layered_config.get('rules_layer', {
            'order_range': [0, 20], 'constant': True, 'depth': 2, 'probability': 100, 'comment_prefix': 'ã€ä¸–ç•Œè§„åˆ™ã€‘'
        })
        timeline_config = layered_config.get('timeline_layer', {
            'order': 21, 'constant': True, 'depth': 3, 'probability': 100, 'comment': 'ã€æ•…äº‹æ€»è§ˆã€‘æ—¶é—´çº¿'
        })
        entity_config = layered_config.get('entity_layer', {
            'order_range': [30, 50], 'constant': True, 'depth': 3, 'probability': 95, 'comment_prefix': 'ã€æ ¸å¿ƒå®ä½“ã€‘'
        })
        event_config = layered_config.get('event_layer', {
            'order_base': 110, 'constant': False, 'depth': 4, 'probability': 80, 'comment_prefix': 'ã€äº‹ä»¶ã€‘'
        })

        # åˆå§‹åŒ–SillyTavern v2æ ¼å¼çš„ä¸–ç•Œä¹¦
        layered_worldbook = {
            "name": "ä¸‰å±‚æ¶æ„ä¸–ç•Œä¹¦",
            "description": "åŸºäºè§„åˆ™å±‚ã€æ—¶é—´çº¿å±‚å’Œäº‹ä»¶å±‚çš„æ™ºèƒ½ä¸–ç•Œä¹¦",
            "entries": []
        }

        current_order = 0

        # 1. è§„åˆ™å±‚æ¡ç›®ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼šorder 0-20ï¼‰
        print("ğŸ“ ç”Ÿæˆè§„åˆ™å±‚æ¡ç›®...")
        rules_order_start = rules_config.get('order_range', [0, 20])[0]

        for i, (rule_type, rule_content) in enumerate(rule_summaries.items()):
            rule_entry = {
                "key": [rule_type, f"{rule_type}è§„åˆ™", f"{rule_type}è®¾å®š"],
                "keysecondary": [],
                "comment": f"{rules_config.get('comment_prefix', 'ã€ä¸–ç•Œè§„åˆ™ã€‘')}{rule_type}",
                "content": rule_content,
                "constant": rules_config.get('constant', True),
                "selective": False,
                "order": rules_order_start + i,
                "position": "before_char",
                "disable": False,
                "addMemo": True,
                "excludeRecursion": False,
                "delayUntilRecursion": False,
                "probability": rules_config.get('probability', 100),
                "useProbability": True,
                "depth": rules_config.get('depth', 2),
                "group": "",
                "groupOverride": False,
                "groupWeight": 100,
                "scanDepth": None,
                "caseSensitive": None,
                "matchWholeWords": None,
                "useGroupScoring": False,
                "automationId": "",
                "role": 0,
                "vectorized": False
            }
            layered_worldbook["entries"].append(rule_entry)
            current_order = max(current_order, rule_entry["order"] + 1)

        # 2. æ—¶é—´çº¿æ€»è§ˆæ¡ç›®ï¼ˆorder 21ï¼‰
        print("ğŸ“ ç”Ÿæˆæ—¶é—´çº¿æ€»è§ˆæ¡ç›®...")
        timeline_order = timeline_config.get('order', 21)

        timeline_entry = {
            "key": ["æ—¶é—´çº¿", "æ•…äº‹æ¢—æ¦‚", "å‰§æƒ…æ€»è§ˆ", "æ•…äº‹å‘å±•"],
            "keysecondary": ["å¹´è¡¨", "å¤§äº‹è®°", "æƒ…èŠ‚å‘å±•"],
            "comment": timeline_config.get('comment', 'ã€æ•…äº‹æ€»è§ˆã€‘æ—¶é—´çº¿'),
            "content": timeline_content,
            "constant": timeline_config.get('constant', True),
            "selective": False,
            "order": timeline_order,
            "position": "before_char",
            "disable": False,
            "addMemo": True,
            "excludeRecursion": False,
            "delayUntilRecursion": False,
            "probability": timeline_config.get('probability', 100),
            "useProbability": True,
            "depth": timeline_config.get('depth', 3),
            "group": "",
            "groupOverride": False,
            "groupWeight": 100,
            "scanDepth": None,
            "caseSensitive": None,
            "matchWholeWords": None,
            "useGroupScoring": False,
            "automationId": "",
            "role": 0,
            "vectorized": False
        }
        layered_worldbook["entries"].append(timeline_entry)
        current_order = max(current_order, timeline_order + 1)

        # 3. æ ¸å¿ƒå®ä½“æ¡ç›®ï¼ˆorder 30-50ï¼‰
        print("ğŸ“ ç”Ÿæˆæ ¸å¿ƒå®ä½“æ¡ç›®...")
        entity_order_start = entity_config.get('order_range', [30, 50])[0]

        for i, (entity_name, entity_content) in enumerate(entity_summaries.items()):
            entity_entry = {
                "key": [entity_name],
                "keysecondary": [],
                "comment": f"{entity_config.get('comment_prefix', 'ã€æ ¸å¿ƒå®ä½“ã€‘')}{entity_name}",
                "content": entity_content,
                "constant": entity_config.get('constant', True),
                "selective": False,
                "order": entity_order_start + i,
                "position": "before_char",
                "disable": False,
                "addMemo": True,
                "excludeRecursion": False,
                "delayUntilRecursion": False,
                "probability": entity_config.get('probability', 95),
                "useProbability": True,
                "depth": entity_config.get('depth', 3),
                "group": "",
                "groupOverride": False,
                "groupWeight": 100,
                "scanDepth": None,
                "caseSensitive": None,
                "matchWholeWords": None,
                "useGroupScoring": False,
                "automationId": "",
                "role": 0,
                "vectorized": False
            }
            layered_worldbook["entries"].append(entity_entry)
            current_order = max(current_order, entity_entry["order"] + 1)

        # 4. äº‹ä»¶å±‚æ¡ç›®ï¼ˆåŸºäºsignificanceåŠ¨æ€è®¡ç®—orderï¼‰
        print("ğŸ“ ç”Ÿæˆäº‹ä»¶å±‚æ¡ç›®...")
        order_base = event_config.get('order_base', 110)

        for event in event_entries:
            # ä»ç°æœ‰äº‹ä»¶æ¡ç›®ä¸­æå–ä¿¡æ¯
            significance = event.get('significance', 5)
            event_order = order_base - (significance * 10)  # é‡è¦æ€§è¶Šé«˜ï¼Œorderè¶Šå°

            # ç¡®ä¿äº‹ä»¶æ¡ç›®ç¬¦åˆSillyTavern v2æ ¼å¼
            event_entry = {
                "key": event.get('key', []),
                "keysecondary": event.get('keysecondary', []),
                "comment": event.get('comment', f"{event_config.get('comment_prefix', 'ã€äº‹ä»¶ã€‘')}æœªçŸ¥äº‹ä»¶"),
                "content": event.get('content', ''),
                "constant": event_config.get('constant', False),
                "selective": True,  # äº‹ä»¶é€šå¸¸ä½¿ç”¨é€‰æ‹©æ€§æ³¨å…¥
                "order": event_order,
                "position": "before_char",
                "disable": False,
                "addMemo": True,
                "excludeRecursion": False,
                "delayUntilRecursion": False,
                "probability": event_config.get('probability', 80),
                "useProbability": True,
                "depth": event_config.get('depth', 4),
                "group": "",
                "groupOverride": False,
                "groupWeight": 100,
                "scanDepth": None,
                "caseSensitive": None,
                "matchWholeWords": None,
                "useGroupScoring": False,
                "automationId": "",
                "role": 0,
                "vectorized": False
            }
            layered_worldbook["entries"].append(event_entry)

        # ä¿å­˜ä¸‰å±‚ä¸–ç•Œä¹¦æ–‡ä»¶
        output_file = self.output_dir / "layered_worldbook.json"
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(layered_worldbook, f, ensure_ascii=False, indent=2)

            print("\n" + "="*80)
            print(f"ğŸ‰ ä¸‰å±‚æ¶æ„ä¸–ç•Œä¹¦ç”Ÿæˆå®Œæˆï¼")
            print(f"ğŸ’¾ ä¿å­˜ä½ç½®: {output_file}")
            print(f"ğŸ“Š æ€»æ¡ç›®æ•°: {len(layered_worldbook['entries'])}")
            print(f"  - è§„åˆ™å±‚: {len(rule_summaries)} ä¸ªè§„åˆ™ç±»å‹")
            print(f"  - æ—¶é—´çº¿å±‚: 1 ä¸ªæ€»è§ˆ")
            print(f"  - å®ä½“å±‚: {len(entity_summaries)} ä¸ªæ ¸å¿ƒå®ä½“")
            print(f"  - äº‹ä»¶å±‚: {len(event_entries)} ä¸ªé‡è¦äº‹ä»¶")
            print("\nğŸ“‹ SillyTavern v2æ ¼å¼éªŒè¯:")
            print(f"  âœ… æ‰€æœ‰æ¡ç›®åŒ…å«å¿…éœ€å­—æ®µ: key, content, order, constant")
            print(f"  âœ… å‚æ•°åˆ†é…ç¬¦åˆä¸‰å±‚æ¶æ„è®¾è®¡")
            print(f"  âœ… ä¼˜å…ˆçº§æ’åº: è§„åˆ™å±‚(0-20) > æ—¶é—´çº¿(21) > å®ä½“(30-50) > äº‹ä»¶(60-120)")
            print("="*80)

            return str(output_file)

        except Exception as e:
            print(f"âŒ ä¿å­˜ä¸‰å±‚ä¸–ç•Œä¹¦å¤±è´¥: {e}")
            return ""

    async def summarize_timeline(self, events: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæ•…äº‹æ—¶é—´çº¿æ€»è§ˆ"""
        if not events:
            return "## æ•…äº‹æ—¶é—´çº¿\n\n*æš‚æ— äº‹ä»¶æ•°æ®*"

        # æ„å»ºæ—¶é—´çº¿æ‘˜è¦
        timeline_summaries = []
        for event in events:
            summary = event.get('event_summary', 'æœªçŸ¥äº‹ä»¶')
            significance = event.get('significance', 5)
            if significance >= 7:  # åªåŒ…å«é‡è¦äº‹ä»¶
                timeline_summaries.append(f"- {summary}")

        timeline_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹å…³é”®äº‹ä»¶åˆ—è¡¨ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æ•…äº‹æ—¶é—´çº¿æ€»è§ˆã€‚

**è¦æ±‚ï¼š**
1. æŒ‰æ—¶é—´é¡ºåºæ¢³ç†ä¸»è¦æƒ…èŠ‚å‘å±•
2. çªå‡ºå…³é”®è½¬æŠ˜ç‚¹å’Œé‡è¦äº‹ä»¶
3. ä¿æŒå™è¿°çš„è¿è´¯æ€§å’Œé€»è¾‘æ€§
4. ä½¿ç”¨Markdownæ ¼å¼ï¼ŒåŒ…å«é€‚å½“çš„æ ‡é¢˜å’Œç»“æ„

**å…³é”®äº‹ä»¶åˆ—è¡¨ï¼š**
{chr(10).join(timeline_summaries[:20])}  # é™åˆ¶é•¿åº¦é¿å…tokenè¶…é™

è¯·ç”Ÿæˆæ•…äº‹æ—¶é—´çº¿æ€»è§ˆï¼š
"""

        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•…äº‹åˆ†æå¸ˆï¼Œæ“…é•¿æ¢³ç†å¤æ‚æƒ…èŠ‚çš„æ—¶é—´çº¿ã€‚"},
                {"role": "user", "content": timeline_prompt}
            ]

            response = await self.client.chat.completions.create(
                model=self.pro_model,
                messages=messages,
                temperature=self.generation_temperature,
                max_tokens=self.max_tokens,
                timeout=self.timeout
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            print(f"âš ï¸ æ—¶é—´çº¿ç”Ÿæˆå¤±è´¥: {e}")
            return f"## æ•…äº‹æ—¶é—´çº¿\n\n*ç”Ÿæˆå¤±è´¥ï¼ŒåŒ…å«{len(events)}ä¸ªäº‹ä»¶*"

    def aggregate_entities_from_events(self, events: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """ä»äº‹ä»¶ä¸­èšåˆå®ä½“ä¿¡æ¯"""
        entities = defaultdict(lambda: {
            'events': [],
            'locations': set(),
            'items': set(),
            'total_significance': 0,
            'event_count': 0,
            'entity_type': 'character'  # é»˜è®¤ä¸ºè§’è‰²
        })

        for event in events:
            participants = event.get('participants', {})
            location = event.get('location', '')
            key_items = event.get('key_items', [])
            significance = event.get('significance', 5)

            # å¤„ç†ä¸»è¦å‚ä¸è€…
            for participant in participants.get('primary', []):
                if participant and participant.strip():
                    entities[participant]['events'].append(event)
                    entities[participant]['total_significance'] += significance
                    entities[participant]['event_count'] += 1
                    if location:
                        entities[participant]['locations'].add(location)
                    entities[participant]['items'].update(key_items)

            # å¤„ç†æ¬¡è¦å‚ä¸è€…
            for participant in participants.get('secondary', []):
                if participant and participant.strip():
                    entities[participant]['events'].append(event)
                    entities[participant]['total_significance'] += significance * 0.5  # æ¬¡è¦å‚ä¸è€…æƒé‡å‡åŠ
                    entities[participant]['event_count'] += 1
                    if location:
                        entities[participant]['locations'].add(location)

        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        for entity_name, entity_data in entities.items():
            entity_data['locations'] = list(entity_data['locations'])
            entity_data['items'] = list(entity_data['items'])
            entity_data['average_significance'] = entity_data['total_significance'] / max(entity_data['event_count'], 1)

        return dict(entities)

    async def summarize_entities(self, entities: Dict[str, Dict[str, Any]]) -> Dict[str, str]:
        """ä¸ºé‡è¦å®ä½“ç”Ÿæˆæ€»ç»“æ¡ç›®"""
        entity_summaries = {}

        # ç­›é€‰é‡è¦å®ä½“ï¼ˆå‚ä¸äº‹ä»¶æ•°é‡ >= 3 æˆ–å¹³å‡é‡è¦æ€§ >= 6ï¼‰
        important_entities = {
            name: data for name, data in entities.items()
            if data['event_count'] >= 3 or data['average_significance'] >= 6
        }

        print(f"ğŸ“Š è¯†åˆ«å‡º {len(important_entities)} ä¸ªé‡è¦å®ä½“")

        for entity_name, entity_data in important_entities.items():
            try:
                # æ„å»ºå®ä½“äº‹ä»¶æ‘˜è¦
                event_summaries = []
                for event in entity_data['events'][:10]:  # é™åˆ¶äº‹ä»¶æ•°é‡
                    summary = event.get('event_summary', '')
                    significance = event.get('significance', 5)
                    event_summaries.append(f"- {summary} (é‡è¦æ€§: {significance})")

                entity_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹äº‹ä»¶ä¿¡æ¯ï¼Œä¸ºè§’è‰²/å®ä½“"{entity_name}"ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æ€»ç»“ã€‚

**å®ä½“ä¿¡æ¯ï¼š**
- å‚ä¸äº‹ä»¶æ•°é‡ï¼š{entity_data['event_count']}
- å¹³å‡é‡è¦æ€§ï¼š{entity_data['average_significance']:.1f}
- æ´»åŠ¨åœ°ç‚¹ï¼š{', '.join(entity_data['locations'][:5])}
- ç›¸å…³ç‰©å“ï¼š{', '.join(entity_data['items'][:5])}

**å‚ä¸çš„å…³é”®äº‹ä»¶ï¼š**
{chr(10).join(event_summaries)}

**è¦æ±‚ï¼š**
1. ç”Ÿæˆä¸€ä»½å®Œæ•´çš„è§’è‰²/å®ä½“æ¡£æ¡ˆ
2. æè¿°å…¶åœ¨æ•…äº‹ä¸­çš„ä½œç”¨å’Œå‘å±•è½¨è¿¹
3. çªå‡ºå…¶é‡è¦æ€§å’Œå½±å“åŠ›
4. ä½¿ç”¨Markdownæ ¼å¼ï¼Œç»“æ„æ¸…æ™°

è¯·ç”Ÿæˆå®ä½“æ€»ç»“ï¼š
"""

                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§’è‰²åˆ†æå¸ˆï¼Œæ“…é•¿ä»äº‹ä»¶ä¸­æç‚¼è§’è‰²ç‰¹å¾å’Œå‘å±•è½¨è¿¹ã€‚"},
                    {"role": "user", "content": entity_prompt}
                ]

                response = await self.client.chat.completions.create(
                    model=self.pro_model,
                    messages=messages,
                    temperature=self.generation_temperature,
                    max_tokens=self.max_tokens,
                    timeout=self.timeout
                )

                entity_summaries[entity_name] = response.choices[0].message.content.strip()
                print(f"âœ… å®Œæˆå®ä½“æ€»ç»“: {entity_name}")

            except Exception as e:
                print(f"âš ï¸ å®ä½“ {entity_name} æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
                entity_summaries[entity_name] = f"## {entity_name}\n\n*æ€»ç»“ç”Ÿæˆå¤±è´¥*"

        return entity_summaries

    def create_event_entries(self, events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¸ºé‡è¦äº‹ä»¶åˆ›å»ºä¸–ç•Œä¹¦æ¡ç›®"""
        event_entries = []

        # ç­›é€‰é‡è¦äº‹ä»¶ï¼ˆsignificance >= 6ï¼‰
        important_events = [
            event for event in events
            if event.get('significance', 5) >= 6
        ]

        print(f"ğŸ“Š è¯†åˆ«å‡º {len(important_events)} ä¸ªé‡è¦äº‹ä»¶")

        for event in important_events:
            try:
                # æ„å»ºå…³é”®è¯åˆ—è¡¨
                keywords = []

                # æ·»åŠ å‚ä¸è€…ä½œä¸ºå…³é”®è¯
                participants = event.get('participants', {})
                keywords.extend(participants.get('primary', []))
                keywords.extend(participants.get('secondary', []))

                # æ·»åŠ åœ°ç‚¹ä½œä¸ºå…³é”®è¯
                location = event.get('location', '')
                if location:
                    keywords.append(location)

                # æ·»åŠ é‡è¦ç‰©å“ä½œä¸ºå…³é”®è¯
                keywords.extend(event.get('key_items', []))

                # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
                keywords = list(set([k.strip() for k in keywords if k and k.strip()]))

                # æ„å»ºäº‹ä»¶æ¡ç›®
                event_entry = {
                    "key": keywords[:5],  # é™åˆ¶å…³é”®è¯æ•°é‡
                    "keysecondary": [],
                    "comment": f"ã€äº‹ä»¶ã€‘{event.get('event_summary', 'æœªçŸ¥äº‹ä»¶')}",
                    "content": self._format_event_content(event),
                    "type": "äº‹ä»¶",
                    "significance": event.get('significance', 5),
                    "event_type": event.get('event_type', 'æœªåˆ†ç±»'),
                    "constant": False,
                    "enabled": True
                }

                event_entries.append(event_entry)

            except Exception as e:
                print(f"âš ï¸ äº‹ä»¶æ¡ç›®åˆ›å»ºå¤±è´¥: {e}")
                continue

        return event_entries

    def _format_event_content(self, event: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–äº‹ä»¶å†…å®¹ä¸ºMarkdown"""
        content_parts = []

        # äº‹ä»¶æ ‡é¢˜
        summary = event.get('event_summary', 'æœªçŸ¥äº‹ä»¶')
        content_parts.append(f"# {summary}")

        # äº‹ä»¶ç±»å‹å’Œé‡è¦æ€§
        event_type = event.get('event_type', 'æœªåˆ†ç±»')
        significance = event.get('significance', 5)
        content_parts.append(f"\n**äº‹ä»¶ç±»å‹**: {event_type}")
        content_parts.append(f"**é‡è¦æ€§**: {significance}/10")

        # å‚ä¸è€…ä¿¡æ¯
        participants = event.get('participants', {})
        if participants.get('primary'):
            content_parts.append(f"\n**ä¸»è¦å‚ä¸è€…**: {', '.join(participants['primary'])}")
        if participants.get('secondary'):
            content_parts.append(f"**æ¬¡è¦å‚ä¸è€…**: {', '.join(participants['secondary'])}")

        # åœ°ç‚¹ä¿¡æ¯
        location = event.get('location', '')
        if location:
            content_parts.append(f"**å‘ç”Ÿåœ°ç‚¹**: {location}")

        # ç›¸å…³ç‰©å“
        key_items = event.get('key_items', [])
        if key_items:
            content_parts.append(f"**ç›¸å…³ç‰©å“**: {', '.join(key_items)}")

        # äº‹ä»¶ç»“æœ
        outcome = event.get('outcome', '')
        if outcome:
            content_parts.append(f"\n**äº‹ä»¶ç»“æœ**: {outcome}")

        # å› æœå…³ç³»
        causal_chain = event.get('causal_chain', {})
        if causal_chain:
            trigger = causal_chain.get('trigger', '')
            consequence = causal_chain.get('consequence', '')
            if trigger:
                content_parts.append(f"\n**è§¦å‘åŸå› **: {trigger}")
            if consequence:
                content_parts.append(f"**åç»­å½±å“**: {consequence}")

        # æƒ…æ„Ÿå½±å“
        emotional_impact = event.get('emotional_impact', '')
        if emotional_impact:
            content_parts.append(f"\n**æƒ…æ„Ÿå½±å“**: {emotional_impact}")

        return '\n'.join(content_parts)

    def save_timeline_worldbook(self, timeline_content: str, entity_summaries: Dict[str, str],
                               event_entries: List[Dict[str, Any]]) -> str:
        """ä¿å­˜äº‹ä»¶é©±åŠ¨çš„æ—¶é—´çº¿ä¸–ç•Œä¹¦"""
        final_worldbook = {
            "name": "äº‹ä»¶é©±åŠ¨ä¸–ç•Œä¹¦",
            "description": "åŸºäºæ—¶é—´çº¿å’Œäº‹ä»¶çš„æ™ºèƒ½ä¸–ç•Œä¹¦",
            "entries": []
        }

        # 1. æ·»åŠ æ—¶é—´çº¿æ€»è§ˆï¼ˆè“ç¯æ¡ç›® - æœ€é«˜ä¼˜å…ˆçº§ï¼‰
        timeline_entry = {
            "key": ["æ—¶é—´çº¿", "æ•…äº‹æ¢—æ¦‚", "å‰§æƒ…æ€»è§ˆ"],
            "keysecondary": ["å¹´è¡¨", "å¤§äº‹è®°"],
            "comment": "ã€æ€»è§ˆã€‘æ•…äº‹æ—¶é—´çº¿",
            "content": timeline_content,
            "type": "æ—¶é—´çº¿æ€»è§ˆ",
            "constant": True,
            "enabled": True
        }
        final_worldbook["entries"].append(timeline_entry)

        # 2. æ·»åŠ å®ä½“æ€»ç»“æ¡ç›®ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        for entity_name, entity_content in entity_summaries.items():
            entity_entry = {
                "key": [entity_name],
                "keysecondary": [],
                "comment": f"ã€æ ¸å¿ƒå®ä½“ã€‘{entity_name}",
                "content": entity_content,
                "type": "æ ¸å¿ƒå®ä½“",
                "constant": True,
                "enabled": True
            }
            final_worldbook["entries"].append(entity_entry)

        # 3. æ·»åŠ é‡è¦äº‹ä»¶æ¡ç›®ï¼ˆç»¿ç¯ï¼‰
        final_worldbook["entries"].extend(event_entries)

        # ä¿å­˜æ–‡ä»¶
        output_file = self.output_dir / "timeline_worldbook.json"
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_worldbook, f, ensure_ascii=False, indent=2)

            print("\n" + "="*60)
            print(f"ğŸ‰ äº‹ä»¶é©±åŠ¨ä¸–ç•Œä¹¦ç”Ÿæˆå®Œæˆï¼")
            print(f"ğŸ’¾ ä¿å­˜ä½ç½®: {output_file}")
            print(f"ğŸ“Š æ€»æ¡ç›®æ•°: {len(final_worldbook['entries'])}")
            print(f"  - æ—¶é—´çº¿æ€»è§ˆ: 1")
            print(f"  - æ ¸å¿ƒå®ä½“: {len(entity_summaries)}")
            print(f"  - é‡è¦äº‹ä»¶: {len(event_entries)}")
            print("="*60)

            return str(output_file)

        except Exception as e:
            print(f"âŒ ä¿å­˜äº‹ä»¶é©±åŠ¨ä¸–ç•Œä¹¦å¤±è´¥: {e}")
            return ""

def main():
    """ä¸»å‡½æ•°"""
    try:
        generator = WorldbookGenerator()
        asyncio.run(generator.generate_worldbook())
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºç°è‡´å‘½é”™è¯¯: {e}")

if __name__ == "__main__":
    main()